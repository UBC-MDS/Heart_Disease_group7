{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the presence of heart disease with key health metrics and attributes\n",
    "\n",
    "by Ethan Fang, Caroline Kahare and Alex Wong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "The objective of this project is to build a classification model that predicts the presence of heart disease based on key health metrics and attributes. It aims to contribute to the understanding and early detection of heart disease, which is crucial for effective medical intervention and prevention. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "The dataset is created by R. Detrano, A. Jánosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, V. Froelicher and was sourced from UC Irvine's Machine Learning Repository. The original dataset consists of 920 observation and 76 attributes, however only 13 attributes were used for the project. The target variable, label, indicates the presence or absence of heart disease, with values ranging from 0 (no presence) to 4 (indicating varying levels of severity). The dataset also contains demographic, clinical, and diagnostic attributes, offering a comprehensive view of patient health metrics.\n",
    "\n",
    "Key features of the dataset include demographic indicators such as age (age in years) and sex (gender: 1 = male, 0 = female). Clinical measurements include trestbps (resting blood pressure), chol (serum cholesterol levels), thalach (maximum heart rate achieved), and oldpeak (ST depression induced by exercise). Additionally, categorical variables such as cp (chest pain type), fbs (fasting blood sugar > 120 mg/dl), restecg (resting electrocardiographic results), exang (exercise-induced angina), slope (slope of the peak exercise ST segment), ca (number of major vessels colored by fluoroscopy), and thal (heart imaging defects) provide valuable context for predicting heart disease. \n",
    "\n",
    "\n",
    "1. **age**: Age in years  \n",
    "2. **sex**: Sex  \n",
    "   - `1` = Male  \n",
    "   - `0` = Female  \n",
    "3. **cp**: Chest pain type  \n",
    "   - Value `1`: Typical angina  \n",
    "   - Value `2`: Atypical angina  \n",
    "   - Value `3`: Non-anginal pain  \n",
    "   - Value `4`: Asymptomatic  \n",
    "4. **trestbps**: Resting blood pressure (in mm Hg on admission to the hospital)  \n",
    "5. **chol**: Serum cholesterol in mg/dl  \n",
    "6. **fbs**: Fasting blood sugar (> 120 mg/dl)  \n",
    "   - `1` = True  \n",
    "   - `0` = False  \n",
    "7. **restecg**: Resting electrocardiographic results  \n",
    "   - Value `0`: Normal  \n",
    "   - Value `1`: Having ST-T wave abnormality (T wave inversions and/or ST  \n",
    "     elevation or depression of > 0.05 mV)  \n",
    "   - Value `2`: Showing probable or definite left ventricular hypertrophy by  \n",
    "     Estes' criteria  \n",
    "8. **thalach**: Maximum heart rate achieved  \n",
    "9. **exang**: Exercise-induced angina  \n",
    "   - `1` = Yes  \n",
    "   - `0` = No  \n",
    "10. **oldpeak**: ST depression induced by exercise relative to rest  \n",
    "11. **slope**: The slope of the peak exercise ST segment  \n",
    "    - Value `1`: Upsloping  \n",
    "    - Value `2`: Flat  \n",
    "    - Value `3`: Downsloping  \n",
    "12. **ca**: Number of major vessels (0-3) colored by fluoroscopy  \n",
    "13. **thal**:  \n",
    "    - `3` = Normal  \n",
    "    - `6` = Fixed defect  \n",
    "    - `7` = Reversible defect\n",
    "14. **label**: \n",
    "    - `0` = Absence \n",
    "    - `1` = Presence \n",
    "    - `2` = Presence\n",
    "    - `3` = Presence\n",
    "    - `4` = Presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair_ally as aly\n",
    "import altair as alt\n",
    "import pandera as pa\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import FeatureLabelCorrelation, FeatureFeatureCorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age','sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>127</td>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>122</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>385</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>920 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp trestbps chol fbs restecg thalach exang oldpeak slope  \\\n",
       "0    28.0  1.0  2.0      130  132   0       2     185     0     0.0   NaN   \n",
       "1    29.0  1.0  2.0      120  243   0       0     160     0     0.0   NaN   \n",
       "2    29.0  1.0  2.0      140  NaN   0       0     170     0     0.0   NaN   \n",
       "3    30.0  0.0  1.0      170  237   0       1     170     0     0.0   NaN   \n",
       "4    31.0  0.0  2.0      100  219   0       1     150     0     0.0   NaN   \n",
       "..    ...  ...  ...      ...  ...  ..     ...     ...   ...     ...   ...   \n",
       "195  54.0  0.0  4.0      127  333   1       1     154     0       0   NaN   \n",
       "196  62.0  1.0  1.0      NaN  139   0       1     NaN   NaN     NaN   NaN   \n",
       "197  55.0  1.0  4.0      122  223   1       1     100     0       0   NaN   \n",
       "198  58.0  1.0  4.0      NaN  385   1       2     NaN   NaN     NaN   NaN   \n",
       "199  62.0  1.0  2.0      120  254   0       2      93     1       0   NaN   \n",
       "\n",
       "      ca thal  label  \n",
       "0    NaN  NaN      0  \n",
       "1    NaN  NaN      0  \n",
       "2    NaN  NaN      0  \n",
       "3    NaN    6      0  \n",
       "4    NaN  NaN      0  \n",
       "..   ...  ...    ...  \n",
       "195  NaN  NaN      1  \n",
       "196  NaN  NaN      0  \n",
       "197  NaN    6      2  \n",
       "198  NaN  NaN      0  \n",
       "199  NaN  NaN      1  \n",
       "\n",
       "[920 rows x 14 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_hungarian = 'data/processed.hungarian.data'\n",
    "file_path_switzerland = 'data/processed.switzerland.data'\n",
    "file_path_cleveland = 'data/processed.cleveland.data'\n",
    "file_path_va = 'data/processed.va.data'\n",
    "hungary_df = pd.read_csv(file_path_hungarian,index_col=False, names = columns)\n",
    "swiss_df = pd.read_csv(file_path_switzerland,index_col=False, names = columns)\n",
    "cleveland_df = pd.read_csv(file_path_cleveland,index_col=False, names = columns)\n",
    "va_df = pd.read_csv(file_path_va,index_col=False, names = columns)\n",
    "\n",
    "# Combine the four dataset into one consolidated set \n",
    "combined_df = pd.concat([hungary_df, swiss_df, cleveland_df, va_df], axis = 0)\n",
    "combined_df.replace(\"?\", np.nan, inplace = True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is in the expected format.\n"
     ]
    }
   ],
   "source": [
    "## --- 1. Correct data file format\n",
    "\n",
    "file_path = [file_path_hungarian, file_path_switzerland, file_path_cleveland, file_path_va]\n",
    "if False in [path.endswith('.data') for path in file_path]:\n",
    "    print(\"Warning: The file extension is not .data\")\n",
    "else:\n",
    "    print(\"File is in the expected format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are correct.\n"
     ]
    }
   ],
   "source": [
    "## --- 2. Correct column names\n",
    "\n",
    "expected_names = set(columns)\n",
    "actual_names = set(combined_df.columns)\n",
    "if expected_names != actual_names:\n",
    "    print(f\"Warning: Column names do not match. Expected: {columns}, Found: {combined_df.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"Column names are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing row found.\n"
     ]
    }
   ],
   "source": [
    "## --- 3. No empty observations\n",
    "\n",
    "empty_obs_schema = pa.DataFrameSchema(\n",
    "    checks = [\n",
    "        pa.Check(lambda df: ~(df.isna().all(axis = 1)).any(), error = \"Empty rows found.\")\n",
    "    ]\n",
    ")\n",
    "try:\n",
    "    empty_obs_schema.validate(combined_df)\n",
    "    print(\"No missing row found.\")\n",
    "except pa.errors.SchemaError as a:\n",
    "    print(f\"Warning: There are {combined_df.isna().sum().sum()} missing values in dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'age' passed the test of missingness.\n",
      "Column 'sex' passed the test of missingness.\n",
      "Column 'cp' passed the test of missingness.\n",
      "Warning: There're too many missing values in column 'trestbps'.\n",
      "Column 'chol' passed the test of missingness.\n",
      "Warning: There're too many missing values in column 'fbs'.\n",
      "Column 'restecg' passed the test of missingness.\n",
      "Warning: There're too many missing values in column 'thalach'.\n",
      "Warning: There're too many missing values in column 'exang'.\n",
      "Warning: There're too many missing values in column 'oldpeak'.\n",
      "Warning: There're too many missing values in column 'slope'.\n",
      "Warning: There're too many missing values in column 'ca'.\n",
      "Warning: There're too many missing values in column 'thal'.\n",
      "Column 'label' passed the test of missingness.\n"
     ]
    }
   ],
   "source": [
    "## --- 4. Missingness not beyond expected threshold\n",
    "\n",
    "threshold = 0.05\n",
    "missing_prop = combined_df.isna().mean()\n",
    "for col, prop in missing_prop.items():\n",
    "    if prop > threshold:\n",
    "        print(f\"Warning: There're too many missing values in column '{col}'.\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' passed the test of missingness.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Validation failed: expected series 'age' to have type int64, got float64\n"
     ]
    }
   ],
   "source": [
    "## --- 5. Correct data types in each column\n",
    "\n",
    "column_type_schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"age\": pa.Column(pa.Int, nullable = True),\n",
    "        \"sex\": pa.Column(pa.Int, nullable = True),\n",
    "        \"cp\": pa.Column(pa.String, nullable = True),\n",
    "        \"trestbps\": pa.Column(pa.Int, nullable = True),\n",
    "        \"chol\": pa.Column(pa.Int, nullable = True),\n",
    "        \"fbs\": pa.Column(pa.Int, nullable = True),\n",
    "        \"restecg\": pa.Column(pa.String, nullable = True),\n",
    "        \"thalach\": pa.Column(pa.Int, nullable = True),\n",
    "        \"exang\": pa.Column(pa.String, nullable = True),\n",
    "        \"oldpeak\": pa.Column(pa.Float, nullable = True),\n",
    "        \"slope\": pa.Column(pa.String, nullable = True),\n",
    "        \"ca\": pa.Column(pa.Float, nullable = True),\n",
    "        \"thal\": pa.Column(pa.String, nullable = True),\n",
    "        \"label\": pa.Column(pa.Int, nullable = True)\n",
    "    }    \n",
    ")\n",
    "try:\n",
    "    column_type_schema.validate(combined_df)\n",
    "    print(\"All columns have correct data types.\")\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(f\"Warning: Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There're duplicate rows: \n",
      "      age  sex   cp trestbps chol fbs restecg thalach exang oldpeak slope  \\\n",
      "101  49.0  0.0  2.0      110  NaN   0       0     160     0     0.0   NaN   \n",
      "102  49.0  0.0  2.0      110  NaN   0       0     160     0     0.0   NaN   \n",
      "139  58.0  1.0  3.0      150  219   0       1     118     1       0   NaN   \n",
      "187  58.0  1.0  3.0      150  219   0       1     118     1       0   NaN   \n",
      "\n",
      "      ca thal  label  \n",
      "101  NaN  NaN      0  \n",
      "102  NaN  NaN      0  \n",
      "139  NaN  NaN      2  \n",
      "187  NaN  NaN      2  .\n"
     ]
    }
   ],
   "source": [
    "## --- 6. No duplicate observations\n",
    "\n",
    "duplicate_obs_schema = pa.DataFrameSchema(\n",
    "    checks=[\n",
    "        pa.Check(lambda df: ~df.duplicated().any(), error=\"There're duplicate rows\")\n",
    "    ]\n",
    ")\n",
    "try:\n",
    "    duplicate_obs_schema.validate(combined_df)\n",
    "    print(\"No duplicate rows found.\")\n",
    "except pa.errors.SchemaError as e:\n",
    "    duplicate_rows = combined_df[combined_df.duplicated(keep=False)]\n",
    "    print(f\"Warning: There're duplicate rows: \\n{duplicate_rows}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There're outlier or anomalous values.\n"
     ]
    }
   ],
   "source": [
    "## --- 7. No outlier or anomalous values\n",
    "\n",
    "values_schema = pa.DataFrameSchema({\n",
    "    \"age\": pa.Column(float, pa.Check.between(0, 120), nullable=True),\n",
    "    \"sex\": pa.Column(float, pa.Check.isin([0.0, 1.0]), nullable=True), \n",
    "    \"cp\": pa.Column(float, pa.Check.isin([1.0, 2.0, 3.0, 4.0]), nullable=True), \n",
    "    \"trestbps\": pa.Column(float, pa.Check.between(20, 220), nullable=True),\n",
    "    \"chol\": pa.Column(float, pa.Check.between(50, 800), nullable=True), \n",
    "    \"fbs\": pa.Column(float, pa.Check.isin([0.0, 1.0]), nullable=True), \n",
    "    \"restecg\": pa.Column(float, pa.Check.isin([0.0, 1.0, 2.0]), nullable=True),  \n",
    "    \"thalach\": pa.Column(float, pa.Check.between(50, 240), nullable=True),  \n",
    "    \"exang\":  pa.Column(float, pa.Check.isin([0.0, 1.0]), nullable=True),  \n",
    "    \"oldpeak\": pa.Column(float, pa.Check.between(0.0, 7.0), nullable=True),  \n",
    "    \"slope\": pa.Column(float, pa.Check.isin([1.0, 2.0, 3.0]), nullable=True),  \n",
    "    \"ca\": pa.Column(float, pa.Check.between(0, 4), nullable=True), \n",
    "    \"thal\": pa.Column(float, pa.Check.isin([3.0, 6.0, 7.0]), nullable=True),  \n",
    "    \"label\": pa.Column(float, pa.Check.between(0.0, 4.0), nullable=True),  \n",
    "})\n",
    "replicate_df = combined_df.applymap(lambda x: float(x) if pd.notnull(x) else x)\n",
    "try:\n",
    "    values_schema.validate(replicate_df, lazy = True)\n",
    "    print(\"No outlier or anomalous value found.\")\n",
    "except pa.errors.SchemaErrors as e:\n",
    "    print(f\"Warning: There're outlier or anomalous values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- 8. There's no categorical value in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class proportions are label\n",
      "0    0.446739\n",
      "1    0.288043\n",
      "2    0.118478\n",
      "3    0.116304\n",
      "4    0.030435\n",
      "Name: proportion, dtype: float64\n",
      "Class proportions are as expected.\n"
     ]
    }
   ],
   "source": [
    "## --- 9. Target/response variable follows expected distribution\n",
    "\n",
    "proportions = combined_df.label.value_counts(normalize=True)\n",
    "print(\"Class proportions are\", proportions)\n",
    "print(\"Class proportions are as expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74921cd065b412dbbdc57e3e4b1ff7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Feature Label Correlation</b></h4>'), HTML(value='<p>Return the PPS (Predict…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is fine.\n"
     ]
    }
   ],
   "source": [
    "## --- 10. No anomalous correlations between target variable and features variables\n",
    "\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "ds = Dataset(combined_df, label='label', cat_features=categorical_features)\n",
    "\n",
    "check_feat_lab_corr = FeatureLabelCorrelation().add_condition_feature_pps_less_than(0.9)\n",
    "check_feat_lab_corr_result = check_feat_lab_corr.run(dataset=ds)\n",
    "check_feat_lab_corr.run(dataset=ds).show()\n",
    "if not check_feat_lab_corr_result.passed_conditions():\n",
    "    raise ValueError(\"The correlation between target and features variables exceeds the threshold.\")\n",
    "else:\n",
    "    print(\"Everything is fine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bf30782837436c8995603f5e3ea13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Feature-Feature Correlation</b></h4>'), HTML(value='<p>    Checks for pairwi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is fine.\n"
     ]
    }
   ],
   "source": [
    "## --- 11. No anomalous correlations between features variables\n",
    "\n",
    "check_feat_feat_corr = FeatureFeatureCorrelation(threshold=0.9)\n",
    "check_feat_feat_corr_result = check_feat_feat_corr.run(dataset=ds)\n",
    "check_feat_feat_corr.run(dataset=ds).show()\n",
    "\n",
    "if not check_feat_feat_corr_result.passed_conditions():\n",
    "    raise ValueError(\"The correlation between features variables exceeds the threshold.\")\n",
    "else:\n",
    "    print(\"Everything is fine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete duplicated rows\n",
    "duplicate_index = [102,187]\n",
    "combined_df = combined_df.drop(index=duplicate_index).reset_index(drop=True)\n",
    "combined_df\n",
    "\n",
    "# Casting continuous features to float64 instead of categories \n",
    "combined_df['trestbps'] = combined_df['trestbps'].astype('float64')\n",
    "combined_df['chol'] = combined_df['chol'].astype('float64')\n",
    "combined_df['thalach'] = combined_df['thalach'].astype('float64')\n",
    "combined_df['oldpeak'] = combined_df['thalach'].astype('float64')\n",
    "\n",
    "# Casting label as categorical \n",
    "combined_df['label'] = combined_df['label'].astype('category')\n",
    "\n",
    "# Casting as categorical \n",
    "combined_df['cp'] = combined_df['cp'].astype('category')\n",
    "combined_df['sex'] = combined_df['sex'].astype('category')\n",
    "\n",
    "# For the following features, have to first convert dtype to number first to ensure the category labels \n",
    "# are not affected by decimals (i.e. 1.0 and 1 are not treated as different groups)\n",
    "combined_df['exang'] = pd.to_numeric(combined_df['exang'], errors='coerce').astype('category')\n",
    "combined_df['thal'] = pd.to_numeric(combined_df['thal'], errors='coerce').astype('category')\n",
    "combined_df['fbs'] = pd.to_numeric(combined_df['fbs'], errors='coerce').astype('category')\n",
    "combined_df['ca'] = pd.to_numeric(combined_df['ca'], errors='coerce').astype('category')\n",
    "combined_df['slope'] = pd.to_numeric(combined_df['slope'], errors='coerce').astype('category')\n",
    "combined_df['restecg'] = pd.to_numeric(combined_df['restecg'], errors='coerce').astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "#### Key findings\n",
    "\n",
    "- Patients with no heart disease exhibits on average higher ST depression induced by exercise relative to rest, higher maximum heart rate and lower serum cholestorel. \n",
    "\n",
    "- Heart disease is more common among patients over 55. \n",
    "\n",
    "- Patients with heart disease are more likely to experience asymptomatic chest pains.\n",
    "\n",
    "- Males appear to be more susceptible to heart disease. \n",
    "\n",
    "- Patients without heart disease tend to have lower fasting blood sugar when compared to the positive group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(combined_df, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aly.alt.data_transformers.enable('vegafusion')\n",
    "#aly.dist(train_df, color='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aly.dist(train_df, dtype = 'category', color = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"label\"])\n",
    "X_test = test_df.drop(columns=[\"label\"])\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak'] # standard scaling for numerical features\n",
    "categorical_features = ['cp', 'restecg'] # onehot encoding for categorical features with > 2 classes\n",
    "binary_features = ['sex', 'exang', 'fbs'] # simple imputing on the binary features\n",
    "drop_features = ['thal', 'ca', 'slope'] # dropping features with signifcant NaN values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer_pipe = make_pipeline(SimpleImputer(strategy = 'median'), StandardScaler())\n",
    "categorical_transfomer_pipe = make_pipeline(SimpleImputer(strategy = 'most_frequent'), OneHotEncoder(drop = 'if_binary', sparse_output = False)) \n",
    "imputer = SimpleImputer(strategy = 'most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer_pipe, numeric_features),\n",
    "    (categorical_transfomer_pipe, categorical_features),\n",
    "    (imputer, binary_features),\n",
    "    (\"drop\", drop_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ( \n",
    "    numeric_features +\n",
    "    preprocessor.named_transformers_['pipeline-2'].get_feature_names_out().tolist() + \n",
    "    binary_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>thalach</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>cp_1.0</th>\n",
       "      <th>cp_2.0</th>\n",
       "      <th>cp_3.0</th>\n",
       "      <th>cp_4.0</th>\n",
       "      <th>restecg_0.0</th>\n",
       "      <th>restecg_1.0</th>\n",
       "      <th>restecg_2.0</th>\n",
       "      <th>sex</th>\n",
       "      <th>exang</th>\n",
       "      <th>fbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.000093</td>\n",
       "      <td>-0.090873</td>\n",
       "      <td>-0.353553</td>\n",
       "      <td>2.030234</td>\n",
       "      <td>2.030234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.651272</td>\n",
       "      <td>0.178458</td>\n",
       "      <td>0.454865</td>\n",
       "      <td>1.236427</td>\n",
       "      <td>1.236427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.216326</td>\n",
       "      <td>-0.090873</td>\n",
       "      <td>-1.849592</td>\n",
       "      <td>0.045715</td>\n",
       "      <td>0.045715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.755028</td>\n",
       "      <td>-1.168198</td>\n",
       "      <td>0.361944</td>\n",
       "      <td>0.045715</td>\n",
       "      <td>0.045715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.066294</td>\n",
       "      <td>0.986452</td>\n",
       "      <td>-0.121249</td>\n",
       "      <td>0.601381</td>\n",
       "      <td>0.601381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>-0.547517</td>\n",
       "      <td>-0.090873</td>\n",
       "      <td>0.426989</td>\n",
       "      <td>0.839523</td>\n",
       "      <td>0.839523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>-0.236251</td>\n",
       "      <td>-1.168198</td>\n",
       "      <td>-1.849592</td>\n",
       "      <td>-1.859423</td>\n",
       "      <td>-1.859423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0.801304</td>\n",
       "      <td>0.986452</td>\n",
       "      <td>-1.849592</td>\n",
       "      <td>-0.867163</td>\n",
       "      <td>-0.867163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0.593793</td>\n",
       "      <td>-1.168198</td>\n",
       "      <td>-1.849592</td>\n",
       "      <td>-1.780042</td>\n",
       "      <td>-1.780042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0.593793</td>\n",
       "      <td>-1.168198</td>\n",
       "      <td>0.371236</td>\n",
       "      <td>0.125096</td>\n",
       "      <td>0.125096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>639 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  trestbps      chol   thalach   oldpeak cp_1.0 cp_2.0 cp_3.0  \\\n",
       "0   -2.000093 -0.090873 -0.353553  2.030234  2.030234    0.0    1.0    0.0   \n",
       "1   -0.651272  0.178458  0.454865  1.236427  1.236427    0.0    0.0    1.0   \n",
       "2    1.216326 -0.090873 -1.849592  0.045715  0.045715    0.0    0.0    0.0   \n",
       "3   -0.755028 -1.168198  0.361944  0.045715  0.045715    0.0    0.0    0.0   \n",
       "4   -1.066294  0.986452 -0.121249  0.601381  0.601381    0.0    1.0    0.0   \n",
       "..        ...       ...       ...       ...       ...    ...    ...    ...   \n",
       "634 -0.547517 -0.090873  0.426989  0.839523  0.839523    0.0    1.0    0.0   \n",
       "635 -0.236251 -1.168198 -1.849592 -1.859423 -1.859423    0.0    0.0    0.0   \n",
       "636  0.801304  0.986452 -1.849592 -0.867163 -0.867163    0.0    0.0    0.0   \n",
       "637  0.593793 -1.168198 -1.849592 -1.780042 -1.780042    0.0    0.0    0.0   \n",
       "638  0.593793 -1.168198  0.371236  0.125096  0.125096    0.0    0.0    0.0   \n",
       "\n",
       "    cp_4.0 restecg_0.0 restecg_1.0 restecg_2.0  sex exang  fbs  \n",
       "0      0.0         1.0         0.0         0.0  0.0   0.0  0.0  \n",
       "1      0.0         1.0         0.0         0.0  0.0   0.0  1.0  \n",
       "2      1.0         1.0         0.0         0.0  1.0   0.0  0.0  \n",
       "3      1.0         0.0         1.0         0.0  1.0   1.0  0.0  \n",
       "4      0.0         1.0         0.0         0.0  0.0   0.0  0.0  \n",
       "..     ...         ...         ...         ...  ...   ...  ...  \n",
       "634    0.0         1.0         0.0         0.0  1.0   0.0  0.0  \n",
       "635    1.0         1.0         0.0         0.0  1.0   0.0  0.0  \n",
       "636    1.0         1.0         0.0         0.0  1.0   1.0  0.0  \n",
       "637    1.0         1.0         0.0         0.0  1.0   0.0  0.0  \n",
       "638    1.0         0.0         0.0         1.0  1.0   1.0  0.0  \n",
       "\n",
       "[639 rows x 15 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns = col_names)\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns = col_names)\n",
    "X_train_transformed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning models application and hyperparameters tuning\n",
    "#### Summary\n",
    "\n",
    "- Various machine learning models were tested and optimized using hyperparameter tuning to identify the best-performing model.\n",
    "- Randomized Search CV was used to perform hyperparameter optimization for each model. Key parameters tuned include:\n",
    "   - Logistic Regression: Regularization strength(C), Solver(liblinear, lbfgs).\n",
    "   - Decision Tree: Maximum depth, Minimum samples per split.\n",
    "   - SVM: Regularization strength(C), Kernel type(linear, rbf).\n",
    "   - KNN: Number of neighbours, Weight type(uniform, distance).\n",
    "\n",
    "- Results:\n",
    "\n",
    "    <img src=\"docs/Best_Models.png\" alt=\"Model Summary Table\" width=\"800\"/>\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "- The K-Nearest Neighbors (KNN) model emerged as the best-performing classifier based on accuracy and weighted F1-score. Despite moderate overall performance, it provided reasonable balance across classes compared to other models.This study demonstrates the potential of leveraging machine learning techniques for predicting heart disease but also highlights the challenges posed by multi-class classification and limited data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#pio.templates.default = \"plotly_white\"\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "#Models for scikit learn\n",
    "\n",
    "#Model Evaluations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state = 123, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state = 123, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "param_distributions = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': stats.loguniform(1e-3, 1e3),\n",
    "        'classifier__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'classifier__max_depth': [3, 5, 10],\n",
    "        'classifier__min_samples_split': stats.randint(2, 20)\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'classifier__C': stats.loguniform(1e-2, 1e2),\n",
    "        'classifier__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'classifier__n_neighbors': stats.randint(3, 20),\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters for Logistic Regression using RandomizedSearchCV...\n",
      "Best parameters for Logistic Regression: {'classifier__C': 0.17670169402947947, 'classifier__solver': 'liblinear'}\n",
      "----------------------------------------\n",
      "Tuning hyperparameters for Decision Tree using RandomizedSearchCV...\n",
      "Best parameters for Decision Tree: {'classifier__max_depth': 10, 'classifier__min_samples_split': 16}\n",
      "----------------------------------------\n",
      "Tuning hyperparameters for Support Vector Machine using RandomizedSearchCV...\n",
      "Best parameters for Support Vector Machine: {'classifier__C': 0.314891164795686, 'classifier__kernel': 'linear'}\n",
      "----------------------------------------\n",
      "Tuning hyperparameters for K-Nearest Neighbors using RandomizedSearchCV...\n",
      "Best parameters for K-Nearest Neighbors: {'classifier__n_neighbors': 9, 'classifier__weights': 'distance'}\n",
      "----------------------------------------\n",
      "Evaluating Logistic Regression on test set...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.92      0.72       106\n",
      "           1       0.43      0.47      0.45        92\n",
      "           2       1.00      0.03      0.05        39\n",
      "           3       0.33      0.10      0.15        30\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.53       274\n",
      "   macro avg       0.47      0.30      0.27       274\n",
      "weighted avg       0.55      0.53      0.45       274\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97  8  0  1  0]\n",
      " [46 43  0  3  0]\n",
      " [12 24  1  2  0]\n",
      " [ 6 21  0  3  0]\n",
      " [ 3  4  0  0  0]]\n",
      "----------------------------------------\n",
      "Evaluating Decision Tree on test set...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70       106\n",
      "           1       0.47      0.49      0.48        92\n",
      "           2       0.17      0.13      0.15        39\n",
      "           3       0.34      0.33      0.34        30\n",
      "           4       0.50      0.14      0.22         7\n",
      "\n",
      "    accuracy                           0.51       274\n",
      "   macro avg       0.43      0.37      0.38       274\n",
      "weighted avg       0.49      0.51      0.50       274\n",
      "\n",
      "Confusion Matrix:\n",
      "[[79 17  3  6  1]\n",
      " [28 45 12  7  0]\n",
      " [10 20  5  4  0]\n",
      " [ 2 10  8 10  0]\n",
      " [ 0  3  1  2  1]]\n",
      "----------------------------------------\n",
      "Evaluating Support Vector Machine on test set...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.91      0.70       106\n",
      "           1       0.44      0.50      0.47        92\n",
      "           2       0.00      0.00      0.00        39\n",
      "           3       0.00      0.00      0.00        30\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.52       274\n",
      "   macro avg       0.20      0.28      0.23       274\n",
      "weighted avg       0.37      0.52      0.43       274\n",
      "\n",
      "Confusion Matrix:\n",
      "[[96 10  0  0  0]\n",
      " [46 46  0  0  0]\n",
      " [18 21  0  0  0]\n",
      " [ 7 23  0  0  0]\n",
      " [ 3  4  0  0  0]]\n",
      "----------------------------------------\n",
      "Evaluating K-Nearest Neighbors on test set...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76       106\n",
      "           1       0.47      0.42      0.45        92\n",
      "           2       0.32      0.21      0.25        39\n",
      "           3       0.29      0.23      0.26        30\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.54       274\n",
      "   macro avg       0.35      0.35      0.34       274\n",
      "weighted avg       0.50      0.54      0.51       274\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93 10  2  1  0]\n",
      " [34 39  7 10  2]\n",
      " [ 5 19  8  5  2]\n",
      " [ 3 13  7  7  0]\n",
      " [ 3  2  1  1  0]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Tuning hyperparameters for {model_name} using RandomizedSearchCV...\")\n",
    "    \n",
    "    clf = Pipeline(steps=[('classifier', model)])\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=clf,\n",
    "        param_distributions=param_distributions[model_name],\n",
    "        scoring=make_scorer(roc_auc_score, needs_proba=True),\n",
    "        n_iter=10, \n",
    "        cv=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    best_models[model_name] = random_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "for model_name, model in best_models.items():\n",
    "    print(f\"Evaluating {model_name} on test set...\")\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{bibliography}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart_disease_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
